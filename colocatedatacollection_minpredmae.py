# -*- coding: utf-8 -*-
"""ColocateDataCollection_minpredmae.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LKIStYwlc80w1wdGm5Dn_mgBaWV6S1Z-
"""

from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import concatenate
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Add
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import SeparableConv2D
from tensorflow.keras.layers import ZeroPadding2D
from tensorflow.keras import Model
import tensorflow as tf
import numpy as np
import time
import random
import json
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.decomposition import PCA
import json


from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.resnet import ResNet101
from tensorflow.keras.applications.resnet import ResNet152
from tensorflow.keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201
from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.mobilenet import MobileNet
from tensorflow.keras.applications.nasnet import NASNetLarge, NASNetMobile
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg19 import VGG19
from tensorflow.keras.applications.xception import Xception
import matplotlib.pyplot as plt

import time

from itertools import permutations
from itertools import combinations

model_builders = {"ResNet50":ResNet50, \
                  "ResNet101":ResNet101, \
                  "ResNet152":ResNet152, \
                  "DenseNet121":DenseNet121, \
                  "DenseNet169":DenseNet169, \
                  "DenseNet201":DenseNet201, \
                  "InceptionResNetV2":InceptionResNetV2, \
                  "InceptionV3" : InceptionV3, \
                  "MobileNet" : MobileNet, \
                  "NASNetLarge" : NASNetLarge, \
                  "NASNetMobile" : NASNetMobile, \
                  "VGG16" : VGG16, \
                  "VGG19" : VGG19, \
                  "Xception" : Xception
                  }

def get_layer_type_to_idx(model_builders,exclude_set = set()):
  layer_to_idx = {'InputLayer':0, 'DepthwiseConv2D': 1, 'SeparableConv2D':1, 'Conv2D':1, 'Add':2, 'Concatenate':2, 'Dense':3}
  for mod in model_builders:
    model = model_builders[mod]()
    for layer in model.layers:
      if layer.__class__.__name__ not in layer_to_idx and layer.__class__.__name__ not in exclude_set:
        layer_to_idx[layer.__class__.__name__] = len(layer_to_idx)
    tf.keras.backend.clear_session()
  return layer_to_idx

def get_layer_metadata(model):
  metadata = []
  for layer in model.layers:
    layer_features = {}
    layer_features['name'] = layer.output.name
    layer_features['count_params'] = layer.count_params()
    layer_features['type'] = layer.__class__.__name__
    #print(layer.output.name,layer.count_params())
    if type(layer.input) == type(list()):
      #print("\t" + str([layer.input[i].name for i in range(len(layer.input))]))
      layer_features['input'] = [layer.input[i].name for i in range(len(layer.input))]
    else:
      #print("\t" + layer.input.name)
      layer_features['input'] = [layer.input.name]
    metadata.append(layer_features)
  return metadata

def build_graph_rep(metadata,layer_type_to_idx):
  attribute_list = []
  name_to_idx = {}
  adj_size = 0
  for i in range(len(metadata)):
    layer_features = metadata[i]
    layer_type = layer_features['type']
    if layer_type in layer_type_to_idx:
      adj_size += 1
  adj_matrix = np.zeros((adj_size,adj_size))
  idx = 0
  for i in range(0, len(metadata)):
    layer_features = metadata[i]
    name = layer_features['name']
    count_params = layer_features['count_params']
    layer_type = layer_features['type']
    layer_inputs = layer_features['input']

    if layer_type in layer_type_to_idx:
      layer_type_idx = layer_type_to_idx[layer_type]
    else:
      for entry in layer_inputs:
        if entry != name:
          name_to_idx[name] = name_to_idx[entry]
          break
      continue

    name_to_idx[name] = idx

    #fill in adjacency matrix
    for entry in layer_inputs:
      if entry != name:
        input_idx = name_to_idx[entry]
        adj_matrix[input_idx][idx] = 1   
    
    #append attribute list
    attribute_list.append((layer_type_idx,count_params))

    idx += 1

  #post process attribute list
  max_layer_type_idx = max([layer_type_to_idx[entry] for entry in layer_type_to_idx])
  for i in range(len(attribute_list)):
    layer_type_idx = attribute_list[i][0]
    count_params = attribute_list[i][1]
    """
    new_entry = np.zeros((max_layer_type_idx+1))
    new_entry[-1] = count_params
    new_entry[layer_type_idx] = 1
    """
    new_entry = np.array([count_params])
    attribute_list[i] = new_entry
  attribute_list = np.array(attribute_list)
  return attribute_list,adj_matrix

def flatten_graph(attr_list,adj_matrix):
    attr_list = attr_list.reshape((attr_list.shape[0]*attr_list.shape[1],1))
    adj_matrix = adj_matrix.reshape((adj_matrix.shape[0]*adj_matrix.shape[1],1))
    return np.concatenate((attr_list,adj_matrix))

#attr_list,adj_matrix = build_graph_rep(get_layer_metadata(model))
#print(flatten_graph(attr_list,adj_matrix).shape)

def create_feature_dict(model_builders, layer_exclude_set = set(), is_adj_matrix = True):
  feature_dict = {}
  layer_type_to_idx = get_layer_type_to_idx(model_builders,layer_exclude_set)
  print(layer_type_to_idx)
  print(layer_exclude_set)
  for mod in model_builders:
    model = model_builders[mod]()
    attr_list,adj_matrix = build_graph_rep(get_layer_metadata(model),layer_type_to_idx)
    if is_adj_matrix is True:
      feature_dict[mod] = (attr_list,adj_matrix)
    else:
      feature_dict[mod] = (attr_list,[])
    tf.keras.backend.clear_session()
  return feature_dict

def preprocess_raw_graph_features(feature_dict):
  largest_graph = 0
  new_feature_dict = {}
  for entry in feature_dict:
    attr_list,adj_matrix = feature_dict[entry]
    if len(attr_list) > largest_graph:
      largest_graph = len(attr_list)
  for entry in feature_dict:
    attr_list,adj_matrix = feature_dict[entry]
    new_attr_list = np.zeros((largest_graph,attr_list.shape[1]))
    new_attr_list[0:attr_list.shape[0]]+= attr_list
    if len(adj_matrix) > 0:
      new_adj_matrix = np.zeros((largest_graph,largest_graph))
      new_adj_matrix[0:adj_matrix.shape[0],0:adj_matrix.shape[1]] += adj_matrix
      new_feature_dict[entry] = flatten_graph(new_attr_list,new_adj_matrix)
    else:
      new_feature_dict[entry] = new_attr_list.reshape((new_attr_list.shape[0]*new_attr_list.shape[1],1))
  return new_feature_dict

feature_dict = preprocess_raw_graph_features(create_feature_dict(model_builders,layer_exclude_set = set(['InputLayer', 'ZeroPadding2D', 'BatchNormalization', 'Activation', 'MaxPooling2D', 'GlobalAveragePooling2D', 'AveragePooling2D', 'Lambda', 'ReLU', 'Reshape', 'Dropout','Cropping2D','Flatten']), is_adj_matrix=False))

for entry in feature_dict:
  print(feature_dict[entry].shape)
  break

def process_indivRuntimes(filename):
  f = open(filename)
  indivRuntimes = {}
  for line in f:
    entry = line.split("\n")[0]
    entry = entry.split("\t")
    indivRuntimes[entry[0]] = float(entry[1])
  return indivRuntimes

indivRuntimesFile = "indivRuntimes.txt"
indivRuntimes = process_indivRuntimes(indivRuntimesFile)

#for TPU runtimes:
#for entry in indivRuntimes:
#  indivRuntimes[entry] /= 1000

"""## Data Setup"""

def indivLatencyData(indivRuntimes,feature_dict):
  features = []
  labels = []
  for mod in indivRuntimes:
    if mod in feature_dict:
      features.append(feature_dict[mod])
      labels.append(indivRuntimes[mod])
  features = np.array(features)
  features = features.reshape((features.shape[0],features.shape[1]))
  return features,labels

def process_indiv_data(filename):
  f = open(filename,"r")
  indivDict = {}
  for line in f:
    entry = line.split('\n')[0]
    entry = entry.split('\t')
    indivDict[entry[0]] = [float(entry[i]) for i in range(1,len(entry))]
  return indivDict

def my_train_test_split(profile_features,features,labels, test_size):
  t_size = int(test_size*len(labels))
  return profile_features[t_size:],features[t_size:],profile_features[0:t_size],features[0:t_size],labels[t_size:],labels[0:t_size]

indiv_profile_filename = "V1.txt"

profile_features,labels = indivLatencyData(indivRuntimes,process_indiv_data(indiv_profile_filename))
features,labels = indivLatencyData(indivRuntimes,feature_dict)
profile_features,features,labels = shuffle(profile_features,features,labels)

#X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)
#base_X_train, X_train, base_X_test, X_test, y_train, y_test = my_train_test_split(profile_features,features, labels, test_size=0.2)

def process_colocate_data(filename,indivRun):
  f = open(filename,"r")
  colocDict = {}
  for line in f:
    entry = line.split('\n')[0]
    entry = entry.split('\t')
    if entry[0] not in colocDict:
      colocDict[entry[0]] = {entry[1]:(float(entry[2])-float(indivRun[entry[0]]))/float(indivRun[entry[0]])}
    else:
      colocDict[entry[0]][entry[1]] = (float(entry[2])-float(indivRun[entry[0]]))/float(indivRun[entry[0]])
  return colocDict

def get_train_data(colocDict,indivDict,prof_indivDict):
  y = []
  x = []
  prof_x = []
  modlist = []
  for mod1 in colocDict:
    for mod2 in colocDict[mod1]:
      y.append(colocDict[mod1][mod2])
      x.append(np.concatenate((indivDict[mod1],indivDict[mod2])))
      prof_x.append(prof_indivDict[mod1] + prof_indivDict[mod2])
      modlist.append((mod1,mod2))
  x = np.array(x)
  x = x.reshape((x.shape[0],x.shape[1]))
  y = np.array(y)
  prof_x = np.array(prof_x)
  print(x.shape)
  return prof_x,x,y,modlist

def process_multicolocate_data(filename,indivRun):
  f = open(filename,"r")
  colocDict = {}
  for line in f:
    entry = line.split('\n')[0]
    entry = entry.split('\t')
    key = []
    for i in range(1,len(entry)-1):
      key.append(entry[i])
    key.sort()
    if entry[0] not in colocDict:
      colocDict[entry[0]] = {tuple(key):(float(entry[-1])-indivRun[entry[0]])/indivRun[entry[0]]}
      #colocDict[entry[0]] = {tuple(key):(float(entry[-1])-indivRun[entry[0]])}
    else:
      colocDict[entry[0]][tuple(key)] = (float(entry[-1]) - indivRun[entry[0]])/indivRun[entry[0]]
      #colocDict[entry[0]][tuple(key)] = (float(entry[-1]) - indivRun[entry[0]])
  return colocDict

def get_train_data_multicolocate(colocDict,indivDict,prof_indivDict):
  y = []
  x = []
  prof_x = []
  multimodlist = []
  for mod1 in colocDict:
    for modlist in colocDict[mod1]:
      for instance_modlist in permutations(modlist,len(modlist)):
        next_ex = indivDict[mod1]
        prof_next_ex = prof_indivDict[mod1]
        for mod in instance_modlist:
          next_ex = np.concatenate((next_ex,indivDict[mod]))
          prof_next_ex = prof_next_ex + prof_indivDict[mod]
        y.append(colocDict[mod1][modlist])
        x.append(next_ex)
        prof_x.append(np.array(prof_next_ex))
        multimodlist.append((mod1,modlist))
  x = np.array(x)
  x = x.reshape((x.shape[0],x.shape[1]))
  y = np.array(y)
  prof_x = np.array(prof_x)
  print(x.shape)
  return np.array(prof_x),np.array(x),np.array(y),multimodlist

def get_train_partition(data,split):
  return data[0:int(split*len(data))]
def get_test_partition(data,split):
  return data[int(split*len(data)):len(data)]

colocDict = process_colocate_data("colocRuntimes.txt",indivRuntimes)
profile_features,features,labels,modlist = get_train_data(colocDict,feature_dict,process_indiv_data(indiv_profile_filename))

from sklearn.ensemble import RandomForestRegressor

def MAE(truth,predictions,names,indivRun):
  total = 0
  for i in range(0,len(truth)):
    total = total + abs(truth[i] - predictions[i])*indivRun[names[i][0]]
    #print(truth[i]*indivRun[names[i]] + indivRun[names[i]],names[i])
  return total / len(truth)

"""## Limited Data Experiments"""

num_trials = 10
this_train_sizes = np.linspace(1/len(labels),1,len(labels))
results = np.zeros((len(this_train_sizes),1))

for n in range(num_trials):
  profile_features,features,labels,modlist = shuffle(profile_features,features,labels,modlist)
  cur_X_train,cur_y_train = profile_features[0:int(np.ceil(len(labels)*this_train_sizes[0]))],labels[0:int(np.ceil(len(labels)*this_train_sizes[0]))]
  available_sample = set([i for i in range(int(np.ceil(len(labels)*this_train_sizes[0])),len(labels))])
  cur_reg = RandomForestRegressor().fit(cur_X_train,cur_y_train)
  results[0] += MAE(labels,cur_reg.predict(profile_features),modlist,indivRuntimes)
  for i in range(1,len(this_train_sizes)):
    samples = []
    sample_costs = []
    num_to_profile = max(1,int(np.floor(len(labels)*(this_train_sizes[i]-this_train_sizes[i-1]))))
    available_list = shuffle(list(available_sample)) #for num_to_profile = 1
    for j in range(min(100,len(available_sample))):
      if num_to_profile == 1:
        new_sample_idx = [available_list[j]]
      else:
        new_sample_idx = random.sample(available_sample,min(num_to_profile,len(available_sample)))
      new_X_train = np.array([profile_features[idx] for idx in new_sample_idx])
      new_y_train = np.array([labels[idx] for idx in new_sample_idx])
      reg = RandomForestRegressor().fit(np.concatenate((cur_X_train,new_X_train)),np.concatenate((cur_y_train,cur_reg.predict(new_X_train))))
      #reg = RandomForestRegressor().fit(np.concatenate((cur_X_train,new_X_train)),np.concatenate((cur_y_train,new_y_train)))
      samples.append((new_X_train,new_y_train,new_sample_idx))
      sample_costs.append(MAE(labels,reg.predict(profile_features),modlist,indivRuntimes))
      
    best_sample_idx = np.argmin(sample_costs)
    available_sample = available_sample - set(samples[best_sample_idx][2])
    cur_X_train = np.concatenate((cur_X_train,samples[best_sample_idx][0]))
    cur_y_train = np.concatenate((cur_y_train,samples[best_sample_idx][1]))
    cur_reg = RandomForestRegressor().fit(cur_X_train,cur_y_train)
    results[i] += MAE(labels,reg.predict(profile_features),modlist,indivRuntimes)
    #results[i] += mean_absolute_error(y_test,reg.predict(base_X_test))
results /=num_trials

json.dump(results.tolist(),open("min_predmae_10sim.json","w"))
json.dump(this_train_sizes.tolist(),open("trainsize_min_predmae_10sim.json","w"))